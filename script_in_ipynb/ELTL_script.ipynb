{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# branch table\n",
    "# set = 20 branches\n",
    "fake = Faker()\n",
    "\n",
    "#### Generating Data Def function ####\n",
    "# Thai cities to random\n",
    "thai_cities = [\n",
    "    'Bangkok', 'Chiang Mai', 'Phuket', 'Khon Kaen', 'Nakhon Ratchasima',\n",
    "    'Ayutthaya', 'Nakhon Si Thammarat', 'Udon Thani', 'Hua Hin', 'Pattaya',\n",
    "    'Sukhothai', 'Surat Thani', 'Ubon Ratchathani', 'Mae Hong Son', 'Lampang',\n",
    "    'Ratchaburi', 'Nakhon Pathom', 'Songkhla', 'Loei', 'Trat', 'Chonburi',\n",
    "    'Samut Prakan', 'Nonthaburi', 'Pathum Thani', 'Prachuap Khiri Khan', 'Phetchaburi',\n",
    "    'Nakhon Nayok', 'Uttaradit', 'Kamphaeng Phet', 'Yasothon', 'Amnat Charoen',\n",
    "    'Roi Et', 'Kalasin', 'Mukdahan', 'Sakon Nakhon', 'Saraburi',\n",
    "    'Suphan Buri', 'Lopburi', 'Nakhon Phanom', 'Chaiyaphum', 'Buriram'\n",
    "]\n",
    "\n",
    "def generate_fake_branch_data(num_records):\n",
    "    branch_data = []\n",
    "    for _ in range(num_records):\n",
    "        branch = {\n",
    "            'branch_type': random.choice(['standalone', 'in_shopping_mall', 'in_community_mall', 'small_shop']),\n",
    "            'city': random.choice(thai_cities),  # Random cities from list\n",
    "            'country': 'Thailand',\n",
    "            'phone_number': fake.phone_number(),\n",
    "            'shop_open_date': fake.date_this_decade()\n",
    "        }\n",
    "        branch_data.append(branch)\n",
    "    return branch_data\n",
    "\n",
    "branches = generate_fake_branch_data(20) # number of records\n",
    "branches_df = pd.DataFrame(branches)\n",
    "\n",
    "# Product to category mapping\n",
    "product_to_category = {\n",
    "    # Fruits\n",
    "    'Apple': 'Fruits',\n",
    "    'Banana': 'Fruits',\n",
    "    'Orange': 'Fruits',\n",
    "    'Grapes': 'Fruits',\n",
    "    'Strawberry': 'Fruits',\n",
    "    'Blueberry': 'Fruits',\n",
    "    'Pineapple': 'Fruits',\n",
    "    'Mango': 'Fruits',\n",
    "    'Peach': 'Fruits',\n",
    "    'Cherry': 'Fruits',\n",
    "\n",
    "    # Dairy\n",
    "    'Milk': 'Dairy',\n",
    "    'Cheese': 'Dairy',\n",
    "    'Yogurt': 'Dairy',\n",
    "    'Butter': 'Dairy',\n",
    "    'Cream': 'Dairy',\n",
    "    'Cottage Cheese': 'Dairy',\n",
    "\n",
    "    # Bakery\n",
    "    'Bread': 'Bakery',\n",
    "    'Croissant': 'Bakery',\n",
    "    'Bagels': 'Bakery',\n",
    "    'Muffins': 'Bakery',\n",
    "    'Donuts': 'Bakery',\n",
    "\n",
    "    # Meat\n",
    "    'Chicken Breast': 'Meat',\n",
    "    'Ground Beef': 'Meat',\n",
    "    'Pork Chops': 'Meat',\n",
    "    'Steak': 'Meat',\n",
    "    'Sausages': 'Meat',\n",
    "\n",
    "    # Grains\n",
    "    'Rice': 'Grains',\n",
    "    'Pasta': 'Grains',\n",
    "    'Quinoa': 'Grains',\n",
    "    'Oats': 'Grains',\n",
    "    'Barley': 'Grains',\n",
    "\n",
    "    # Beverages\n",
    "    'Soda': 'Beverages',\n",
    "    'Orange Juice': 'Beverages',\n",
    "    'Coffee': 'Beverages',\n",
    "    'Tea': 'Beverages',\n",
    "    'Water': 'Beverages',\n",
    "\n",
    "    # Snacks\n",
    "    'Chips': 'Snacks',\n",
    "    'Pretzels': 'Snacks',\n",
    "    'Nuts': 'Snacks',\n",
    "    'Granola Bars': 'Snacks',\n",
    "    'Cookies': 'Snacks',\n",
    "\n",
    "    # Frozen Foods\n",
    "    'Frozen Pizza': 'Frozen Foods',\n",
    "    'Ice Cream': 'Frozen Foods',\n",
    "    'Frozen Vegetables': 'Frozen Foods',\n",
    "    'Frozen Fries': 'Frozen Foods',\n",
    "    'Frozen Berries': 'Frozen Foods',\n",
    "\n",
    "    # Condiments\n",
    "    'Ketchup': 'Condiments',\n",
    "    'Mustard': 'Condiments',\n",
    "    'Mayonnaise': 'Condiments',\n",
    "    'Barbecue Sauce': 'Condiments',\n",
    "    'Soy Sauce': 'Condiments',\n",
    "\n",
    "    # Household Supplies\n",
    "    'Toilet Paper': 'Household Supplies',\n",
    "    'Paper Towels': 'Household Supplies',\n",
    "    'Laundry Detergent': 'Household Supplies',\n",
    "    'Dish Soap': 'Household Supplies',\n",
    "    'Cleaning Spray': 'Household Supplies',\n",
    "\n",
    "    # Personal Care\n",
    "    'Shampoo': 'Personal Care',\n",
    "    'Conditioner': 'Personal Care',\n",
    "    'Toothpaste': 'Personal Care',\n",
    "    'Soap': 'Personal Care',\n",
    "    'Deodorant': 'Personal Care',\n",
    "\n",
    "    # Health Foods\n",
    "    'Almond Milk': 'Health Foods',\n",
    "    'Chia Seeds': 'Health Foods',\n",
    "    'Spirulina': 'Health Foods',\n",
    "    'Acai Berries': 'Health Foods',\n",
    "    'Goji Berries': 'Health Foods'\n",
    "}\n",
    "\n",
    "def generate_all_products_data():\n",
    "    product_data = []\n",
    "\n",
    "    for product_name, category_name in product_to_category.items():\n",
    "        product = {\n",
    "            'product_name': product_name,\n",
    "            'category_name': category_name\n",
    "\n",
    "        }\n",
    "        product_data.append(product)\n",
    "\n",
    "    return product_data\n",
    "\n",
    "products = generate_all_products_data()\n",
    "products_df = pd.DataFrame(products)\n",
    "\n",
    "# customers_table\n",
    "# set 1000 customers\n",
    "\n",
    "def generate_fake_customers_data(num_records):\n",
    "    customer_data = []\n",
    "    \n",
    "    for _ in range(num_records):\n",
    "        customer = {\n",
    "            'first_name': fake.first_name(),\n",
    "            'last_name': fake.last_name(),\n",
    "            'email': fake.email(),\n",
    "            'phone': fake.phone_number(),\n",
    "            'address': fake.address(),\n",
    "            'city': fake.city(),\n",
    "            'country': fake.country(),\n",
    "            'registration_date': fake.date_this_decade()\n",
    "        }\n",
    "        customer_data.append(customer)\n",
    "    \n",
    "    return customer_data\n",
    "\n",
    "customers = generate_fake_customers_data(1000)\n",
    "customers_df = pd.DataFrame(customers)\n",
    "\n",
    "# employees_table\n",
    "# set = 100 employees\n",
    "def generate_employees_data(num_records=50):\n",
    "    employees_data = []\n",
    "    \n",
    "    for _ in range(num_records):\n",
    "        employee = {\n",
    "            'first_name': fake.first_name(),\n",
    "            'last_name': fake.last_name(),\n",
    "            'position': random.choice(['Manager', 'Sales Associate', 'Cashier', 'Stock Clerk', 'Supervisor']),\n",
    "            'branch_id': random.randint(1, 20),  # 20 branches\n",
    "            'salary': round(random.uniform(30000, 90000), 2),\n",
    "            'hire_date': fake.date_this_decade(),\n",
    "            'termination_date': fake.date_between(start_date='-1y', end_date='today') if random.choice([True, False]) else None,\n",
    "            'email': fake.email(),\n",
    "            'phone': fake.phone_number(),\n",
    "            'is_active': random.choice([0, 1])\n",
    "        }\n",
    "        employees_data.append(employee)\n",
    "    \n",
    "    return employees_data\n",
    "\n",
    "employees = generate_employees_data(100)\n",
    "\n",
    "employees_df = pd.DataFrame(employees)\n",
    "\n",
    "# inventory table\n",
    "def create_inventory_data(num_branches, num_products):\n",
    "    data = {\n",
    "        'product_id': [],\n",
    "        'branch_id': [],\n",
    "        'quantity': [],\n",
    "        'last_updated': []\n",
    "    }\n",
    "\n",
    "    for branch in range(1, num_branches + 1):\n",
    "        for product in range(1, num_products + 1):\n",
    "            data['product_id'].append(product)\n",
    "            data['branch_id'].append(branch)\n",
    "            data['quantity'].append(random.randint(0, 1000))  \n",
    "            data['last_updated'].append(fake.date_this_year()) \n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "inventory_df = create_inventory_data(20, 66)\n",
    "\n",
    "# suppliers 20\n",
    "def generate_fake_supplier_data(num_records):\n",
    "    supplier_data = []\n",
    "    \n",
    "    for _ in range(num_records):\n",
    "        supplier = {\n",
    "            'supplier_name': fake.company(),\n",
    "            'contact_name': fake.name(),\n",
    "            'contact_email': fake.email(),\n",
    "            'contact_phone': fake.phone_number(),\n",
    "            'address': fake.address(),\n",
    "            'city': fake.city(),\n",
    "            'country': fake.country()\n",
    "        }\n",
    "        supplier_data.append(supplier)\n",
    "    \n",
    "    return supplier_data\n",
    "\n",
    "suppliers = generate_fake_supplier_data(20)\n",
    "suppliers_df = pd.DataFrame(suppliers)\n",
    "\n",
    "## order \n",
    "def generate_mock_orders(num_orders):\n",
    "    orders = []\n",
    "    for _ in range(num_orders):\n",
    "        order = {\n",
    "            'order_id' : np.nan,\n",
    "            \"customer_id\": random.randint(1, 1000),  # 1000 customers for members\n",
    "            # payment_id is auto increment in database\n",
    "            \"employee_id\": random.randint(1, 50),  # 50 employees\n",
    "            \"branch_id\": random.randint(1, 20),  # 20 branches\n",
    "            \"product_id\": random.randint(1, 66),  # 66 products\n",
    "            \"order_date\": fake.date_this_decade(),\n",
    "            \"amount\": round(random.uniform(5.00, 500.00), 2),\n",
    "            \"quantities\": random.randint(1, 30),\n",
    "            \"payment_method\": random.choice(['credit_card', 'paypal', 'bank_transfer', 'cash']),\n",
    "            \"transaction_id\": fake.uuid4(),  # Unique transaction ID\n",
    "            \"billing_address\": fake.street_address(),\n",
    "            \"billing_city\": fake.city(),\n",
    "            \"billing_country\": fake.country()\n",
    "        }\n",
    "        orders.append(order)\n",
    "    return orders\n",
    "orders_data = generate_mock_orders(random.randint(500, 1500))\n",
    "orders_df = pd.DataFrame(orders_data)\n",
    "\n",
    "## Supplier\n",
    "def generate_mock_supplier_orders(num_orders):\n",
    "    supplier_orders = []\n",
    "    for _ in range(num_orders):\n",
    "        order = {\n",
    "            'supplier_purchase_order_id' : np.nan,\n",
    "            \"supplier_id\": random.randint(1, 20),\n",
    "            \"product_id\": random.randint(1, 66),   # 66 products\n",
    "            \"employee_id\": random.randint(1, 50),\n",
    "            \"branch_id\": random.randint(1, 20),\n",
    "            \"order_date\": fake.date_between(start_date='-2y', end_date='today'),\n",
    "            \"arrival_date\": fake.date_between(start_date='-2y', end_date='today'),\n",
    "            \"quantity\": random.randint(10, 500),\n",
    "            \"unit_price\": round(random.uniform(5.0, 100.0), 2),\n",
    "        }\n",
    "        supplier_orders.append(order)\n",
    "    return supplier_orders\n",
    "supplier_orders_data = generate_mock_supplier_orders(random.randint(50, 300))\n",
    "supplier_orders = pd.DataFrame(supplier_orders_data)\n",
    "supplier_orders_data_df = pd.DataFrame(supplier_orders_data)\n",
    "##### End Generate Data def function #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DATABASE  ##############\n",
    "# Import packages for Database connection code \n",
    "import os\n",
    "import json\n",
    "import mysql.connector  \n",
    "from sqlalchemy import create_engine  \n",
    "import pandas as pd  \n",
    "\n",
    "# # Get database Credential ## \n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "json_file_path = os.path.join(current_dir, 'mysql_db_acc_detail.json')\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    db_details = json.load(json_file)\n",
    "\n",
    "database = db_details.get('database')\n",
    "user = db_details.get('user')\n",
    "password = db_details.get('password')\n",
    "host = db_details.get('host')\n",
    "port = db_details.get('port')\n",
    "\n",
    "# append df to my sql\n",
    "def append_dataframe_to_mysql(df, table_name, host, database, user, password):\n",
    "    connection_string = f\"mysql+mysqlconnector://{user}:{password}@{host}/{database}\"\n",
    "    engine = create_engine(connection_string)\n",
    "    \n",
    "    df.to_sql(name=table_name, con=engine, if_exists='append', index=False)\n",
    "        \n",
    "############### ETL def fucntion #############\n",
    "import mysql.connector\n",
    "import json\n",
    "\n",
    "import json\n",
    "\n",
    "def get_db_config(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            config = json.load(file)\n",
    "        return config\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {file_path} was not found.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Failed to decode JSON from {file_path}.\")\n",
    "        return None\n",
    "\n",
    "def get_latest_id(column, table, filename=\"mysql_db_acc_detail.json\"):\n",
    "    \"\"\"\n",
    "    Get the latest (maximum) ID from the specified column and table.\n",
    "    \"\"\"\n",
    "    config = get_db_config(filename)\n",
    "\n",
    "    if not config:\n",
    "        return None  # Exit if config loading failed\n",
    "\n",
    "    try:\n",
    "        connection = mysql.connector.connect(\n",
    "            host=config['host'], \n",
    "            database=config['database'], \n",
    "            user=config['user'], \n",
    "            password=config['password']\n",
    "        )\n",
    "\n",
    "        if connection.is_connected():\n",
    "            cursor = connection.cursor()\n",
    "\n",
    "            query = f\"SELECT MAX({column}) FROM {table}\"\n",
    "            cursor.execute(query)\n",
    "            max_id = cursor.fetchone()[0]\n",
    "            return max_id  # Return the maximum ID\n",
    "\n",
    "    except mysql.connector.Error as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    finally:\n",
    "        if connection.is_connected():\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "############## END ETL def fucntion #############\n",
    "########### END DATABASE #######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import json\n",
    "\n",
    "def get_db_config(file):\n",
    "    try:\n",
    "        with open(file, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Configuration file not found.\")\n",
    "        return None\n",
    "\n",
    "def get_max_id(column, table):\n",
    "    config = get_db_config(\"mysql_db_acc_detail.json\")\n",
    "    if not config:\n",
    "        return None  # Exit if config loading failed\n",
    "\n",
    "    try:\n",
    "        connection = mysql.connector.connect(\n",
    "            host=config['host'], \n",
    "            database=config['database'], \n",
    "            user=config['user'], \n",
    "            password=config['password']\n",
    "        )\n",
    "\n",
    "        if connection.is_connected():\n",
    "            cursor = connection.cursor()\n",
    "\n",
    "            # Use backticks to avoid syntax error with reserved keywords\n",
    "            query = f\"SELECT MAX({column}) FROM `{table}`\"\n",
    "            cursor.execute(query)\n",
    "            max_id = cursor.fetchone()[0]\n",
    "\n",
    "            cursor.close()  # Close the cursor\n",
    "            connection.close()  # Close the connection\n",
    "\n",
    "            return max_id  # Return the maximum ID\n",
    "\n",
    "    except mysql.connector.Error as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Google drive def function #########################\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import pickle\n",
    "import logging\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from datetime import datetime\n",
    "\n",
    "## Def function ##\n",
    "def upload_file_to_drive(file_name, folder_id):\n",
    "    \"\"\"\n",
    "    Upload a file to Google Drive.\n",
    "    :param file_name: The name of the file to upload.\n",
    "    :param folder_id: The Google Drive folder ID where the file will be uploaded.\n",
    "    \"\"\"\n",
    "    creds = None\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'google_drive_api_credential.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        \n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    # Build the Google Drive API service\n",
    "    drive_service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "    # Create a file metadata with the folder ID\n",
    "    file_metadata = {\n",
    "        'name': os.path.basename(file_name),  # Get just the filename for upload\n",
    "        'parents': [folder_id]\n",
    "    }\n",
    "\n",
    "    # Read the file content\n",
    "    media = MediaFileUpload(file_name, mimetype='application/sql')\n",
    "\n",
    "    # Upload the file\n",
    "    try:\n",
    "        file = drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "        timestamp = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "        return f'Upload supermarket backup successfully in Google Drive on {timestamp}.'\n",
    "    except Exception as e:\n",
    "        return f\"Error uploading file to Google Drive: {e}\"\n",
    "def get_folder_id_by_name(folder_name, parent_folder_id):\n",
    "    \"\"\"\n",
    "    Search for a folder by its name inside a parent folder and return its folder ID.\n",
    "    :param folder_name: The name of the folder to search for.\n",
    "    :param parent_folder_id: The Google Drive folder ID where the search will be conducted.\n",
    "    :return: The folder ID of the found folder or None if not found.\n",
    "    \"\"\"\n",
    "    creds = None\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    \n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'google_drive_api_credential.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        \n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    # Build the Google Drive API service\n",
    "    drive_service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "    # Search for the folder inside the parent folder\n",
    "    query = f\"name = '{folder_name}' and '{parent_folder_id}' in parents and mimeType = 'application/vnd.google-apps.folder'\"\n",
    "    results = drive_service.files().list(q=query, fields=\"files(id, name)\").execute()\n",
    "    folders = results.get('files', [])\n",
    "\n",
    "    if folders:\n",
    "        # If a folder is found, return its ID\n",
    "        return folders[0]['id']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "################## END Google drive def function #########################\n",
    "################## Send e-mail ###################\n",
    "import smtplib\n",
    "from datetime import datetime\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "import logging\n",
    "\n",
    "def send_success_email(to_emails,minute,second):\n",
    "    sender_email = \"kmaungmanee96@gmail.com\"\n",
    "    password = \"zmfh bbcb oucg uoqr\"  # Use your App Password\n",
    "    smtp_server = \"smtp.gmail.com\"\n",
    "    port = 587  # For starttls\n",
    "\n",
    "    # Prepare the email content\n",
    "    current_date = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "    \n",
    "    # Define the subject and body\n",
    "    subject = f\"ELT Process Completion Report - {current_date}\"\n",
    "    body = f\"\"\"\n",
    "    Hello Team,\n",
    "\n",
    "    The ELT process has been successfully completed today, {current_date}.\n",
    "    ELT Process time taken is {minute} minutes and {second} seconds.\n",
    "\n",
    "    Best regards,\n",
    "    Kittisak M.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the email message\n",
    "    message = MIMEMultipart()\n",
    "    message['From'] = sender_email\n",
    "    message['To'] = \", \".join(to_emails)  # Join recipients with commas\n",
    "    message['Subject'] = subject\n",
    "    message.attach(MIMEText(body, 'plain'))\n",
    "    \n",
    "    try:\n",
    "        server = smtplib.SMTP(smtp_server, port)\n",
    "        server.starttls()  # Secure the connection\n",
    "        server.login(sender_email, password)\n",
    "        server.sendmail(sender_email, to_emails, message.as_string())\n",
    "        print(\"Complete report e-mail sent successfully!\")\n",
    "        logging.info(\"Complete report e-mail sent successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to send complete report email: {e}\")\n",
    "        logging.error(f\"Failed to send complete report email: {e}\")\n",
    "    finally:\n",
    "        server.quit()\n",
    "\n",
    "############## send Failed Email ###############\n",
    "def send_failed_email(to_emails, error_message):\n",
    "    sender_email = \"kmaungmanee96@gmail.com\"\n",
    "    password = \"zmfh bbcb oucg uoqr\"  # Use your App Password\n",
    "    smtp_server = \"smtp.gmail.com\"\n",
    "    port = 587  # For starttls\n",
    "\n",
    "    # Prepare the email content\n",
    "    current_date = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "    \n",
    "    # Define the subject and body\n",
    "    subject = f\"ELT Process Failure Report - {current_date}!\"\n",
    "    body = f\"\"\"\n",
    "    Hello Team,\n",
    "\n",
    "    An error occurred during the ELT process on {current_date}. \n",
    "    Error Message: {error_message[:100]}.\n",
    "    Please contact [Name] at 097-xxxxxxx or via email at example@gmail.com for assistance.\n",
    "\n",
    "    Best regards,\n",
    "    Kittisak M.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the email message\n",
    "    message = MIMEMultipart()\n",
    "    message['From'] = sender_email\n",
    "    message['To'] = \", \".join(to_emails)\n",
    "    message['Subject'] = subject\n",
    "    message.attach(MIMEText(body, 'plain'))\n",
    "    \n",
    "    try:\n",
    "        # Set up the SMTP server and send the email\n",
    "        server = smtplib.SMTP(smtp_server, port)\n",
    "        server.starttls()  # Upgrade to a secure connection\n",
    "        server.login(sender_email, password)\n",
    "        server.sendmail(sender_email, to_emails, message.as_string())\n",
    "        \n",
    "        print(\"Failure report email sent successfully!\")\n",
    "        logging.info(\"Failure report email sent successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to send failure report email: {e}\")\n",
    "        logging.error(f\"Failed to send failure report email: {e}\")\n",
    "    finally:\n",
    "        # Ensure the server is quit properly, only if it was initialized\n",
    "        if 'server' in locals():\n",
    "            server.quit()\n",
    "################## END Send e-mail ###################\n",
    "\n",
    "################## Delete File #######################\n",
    "def delete_file(file_name, folder_id):\n",
    "    \"\"\"\n",
    "    Deletes a file from Google Drive if it exists in the specified folder.\n",
    "\n",
    "    Parameters:\n",
    "        file_name (str): The name of the file to delete.\n",
    "        folder_id (str): The ID of the folder to search in.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the file was deleted, False if not found.\n",
    "    \"\"\"\n",
    "    # Search for the file by name within the specified folder\n",
    "    query = f\"name='{file_name}' and '{folder_id}' in parents and trashed=false\"\n",
    "    \n",
    "    try:\n",
    "        results = service.files().list(q=query, spaces='drive', pageSize=10,\n",
    "                                       fields=\"nextPageToken, files(id, name)\").execute()\n",
    "        items = results.get('files', [])\n",
    "        \n",
    "        if items:\n",
    "            # If the file exists, delete it\n",
    "            for item in items:\n",
    "                file_id = item['id']\n",
    "                service.files().delete(fileId=file_id).execute()\n",
    "                print(f\"Deleted file '{file_name}' with ID '{file_id}' from the specified folder.\")\n",
    "                return True  # Return True after deleting\n",
    "        else:\n",
    "            print(f\"File '{file_name}' does not exist in the specified folder.\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return False\n",
    "################## End  #######################    \n",
    "\n",
    "################## Storage check ###################\n",
    "######### Local Storage ###########\n",
    "import shutil\n",
    "\n",
    "def check_disk_space(path=\"/\"):\n",
    "    total, used, free = shutil.disk_usage(path)\n",
    "    \n",
    "    print(f\"Total: {total // (2**30)} GiB\")\n",
    "    logging.info(f\"Total: {total // (2**30)} GiB\")\n",
    "    print(f\"Used: {used // (2**30)} GiB\")\n",
    "    logging.info(f\"Used: {used // (2**30)} GiB\")\n",
    "    print(f\"Free: {free // (2**30)} GiB\")\n",
    "    logging.info(f\"Free: {free // (2**30)} GiB\")\n",
    "    print(f\"Percentage Used: {used / total * 100:.2f}%\")\n",
    "    logging.info(f\"Percentage Used: {used / total * 100:.2f}%\")\n",
    "    \n",
    "\n",
    "    percentage_used = used / total * 100\n",
    "\n",
    "    # Check if used space exceeds 95%\n",
    "    if percentage_used > 95:\n",
    "        message = \"Critical Warning: More than 95% of the disk space is used!\"\n",
    "        logging.warning(message)\n",
    "        print(message)\n",
    "    elif percentage_used > 90:\n",
    "        message = \"Warning: More than 90% of the disk space is used!\"\n",
    "        logging.warning(message)\n",
    "        print(message)\n",
    "    else:\n",
    "        message = \"Disk space usage is within acceptable limits.\"\n",
    "        logging.info(message)\n",
    "        print(message)\n",
    "\n",
    "######################################\n",
    "\n",
    "########## Google Drive Storage #########\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Define the scopes\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive.readonly']\n",
    "\n",
    "def authenticate_google_drive():\n",
    "    \"\"\"Authenticate and return the Google Drive service.\"\"\"\n",
    "    creds = None\n",
    "    # The file token.pickle stores the user's access and refresh tokens.\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file('google_drive_api_credential.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    return build('drive', 'v3', credentials=creds)\n",
    "\n",
    "def check_google_drive_space(service):\n",
    "    \"\"\"Check the user's Google Drive space.\"\"\"\n",
    "    about = service.about().get(fields=\"storageQuota\").execute()\n",
    "    quota = about['storageQuota']\n",
    "    \n",
    "    total_space = int(quota['limit']) // (1024 ** 3)  # Convert to GiB\n",
    "    used_space = int(quota['usage']) // (1024 ** 3)    # Convert to GiB\n",
    "    free_space = total_space - used_space\n",
    "\n",
    "    # Log and print disk usage information\n",
    "    logging.info(f\"Total: {total_space} GiB\")\n",
    "    logging.info(f\"Used: {used_space} GiB\")\n",
    "    logging.info(f\"Free: {free_space} GiB\")\n",
    "    percentage_used = (used_space / total_space) * 100\n",
    "    logging.info(f\"Percentage Used: {percentage_used:.2f}%\")\n",
    "\n",
    "    # Check the used space conditions\n",
    "    if percentage_used > 95:\n",
    "        message = \"Critical Warning: More than 95% of Google Drive space is used!\"\n",
    "        logging.warning(message)\n",
    "        print(message)\n",
    "    elif percentage_used > 90:\n",
    "        message = \"Warning: More than 90% of Google Drive space is used!\"\n",
    "        logging.warning(message)\n",
    "        print(message)\n",
    "    else:\n",
    "        message = \"Google Drive space usage is within acceptable limits.\"\n",
    "        logging.info(message)\n",
    "        print(message)\n",
    "\n",
    "######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 475 GiB\n",
      "Used: 188 GiB\n",
      "Free: 287 GiB\n",
      "Percentage Used: 39.55%\n",
      "Disk space usage is within acceptable limits.\n",
      "Google Drive space usage is within acceptable limits.\n",
      "Starting ELT process on 26-10-2024 ....\n",
      "Extracting data from python faker() generated on 26-10-2024 ....\n",
      "Extracting data from python faker() successfully.  Time taken:0 minutes, 0 seconds, and 713 milliseconds.\n",
      "Saving Raw data to 'C:\\supermarket\\raw_data'...\n",
      "raw_branch_26-10-2024 saved successfully.\n",
      "raw_customer_26-10-2024 saved successfully.\n",
      "raw_employee_26-10-2024 saved successfully.\n",
      "raw_inventory_26-10-2024 saved successfully.\n",
      "raw_supplier_26-10-2024 saved successfully.\n",
      "raw_customer_order_26-10-2024 saved successfully.\n",
      "raw_product_26-10-2024 saved successfully.\n",
      "raw_supplier_order_data_26-10-2024 saved successfully.\n",
      "All raw data files have been saved to C:\\\\supermarket\\\\raw_data on 26-10-2024 successfully.\n",
      "END Saving Raw data to 'C:\\supermarket\\raw_data'...\n",
      "Saving Raw data files to Google Drive...\n",
      "Getting Google Drive folder id... \n",
      "Successfully retrieved 'supermarket' folder ID: 1ZO3maUckkFDKJuQo_fU2bvztplk0S-Xb\n",
      "Successfully retrieved 'log' folder ID: 1Yo0R25si2Szwwx9TTkXWZNZqbqR4FxJr\n",
      "Successfully retrieved 'raw_data' folder ID: 1b0gA6ALQcirQt0N_fXbwJxe4gsIfJFtW\n",
      "Successfully retrieved 'transformed_data' folder ID: 1xjqH4tuwmgFEGUcvIHkvmvpu_tHEqQvO\n",
      "Successfully get the folder id in Google Drive\n",
      "Uploading raw data files to Google Drive 'supermarket\\raw_data'...\n",
      "Successfully uploaded raw_branches_csv to Google Drive on 26-10-2024\n",
      "Successfully uploaded raw_customers_csv to Google Drive on 26-10-2024\n",
      "Successfully uploaded raw_employees_csv to Google Drive on 26-10-2024\n",
      "Successfully uploaded raw_inventory_csv to Google Drive on 26-10-2024\n",
      "Successfully uploaded raw_supplier_csv to Google Drive on 26-10-2024\n",
      "Successfully uploaded raw_orders_csv to Google Drive on 26-10-2024\n",
      "Successfully uploaded raw_product_csv to Google Drive on 26-10-2024\n",
      "Successfully uploaded raw_supplier_order_csv to Google Drive on 26-10-2024\n",
      "All raw data files on 26-10-2024 saved to Google Drive successfully.\n",
      "END Saving Raw data. Time taken:0 minutes, 22 seconds, and 650 milliseconds.\n",
      "Transforming data....\n",
      "END transforming data. Time taken:0 minutes, 0 seconds, and 57 milliseconds.\n",
      "Saving Transformed data...\n",
      "Saving Transformed data to C:\\\\supermarket\\\\transformed_data\n",
      "Transformed data has been saved to C:\\\\supermarket\\\\transformed_data successfully.\n",
      "END Saving Transformed data to C:\\\\supermarket\\\\transformed_data'\n",
      "Uploading transformed data files to Google Drive 'supermarket\\transformed_data'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kmaun\\AppData\\Local\\Temp\\ipykernel_35008\\1365823226.py:266: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  main_orders_df['payment_id'] = main_orders_df['payment_id'].combine_first(pd.Series(new_payment_ids))\n",
      "C:\\Users\\Kmaun\\AppData\\Local\\Temp\\ipykernel_35008\\1365823226.py:279: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  main_orders_df['order_id'] = main_orders_df['order_id'].combine_first(pd.Series(new_order_ids))\n",
      "C:\\Users\\Kmaun\\AppData\\Local\\Temp\\ipykernel_35008\\1365823226.py:287: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  supplier_orders_data_df['supplier_purchase_order_id'] = supplier_orders_data_df['supplier_purchase_order_id'].combine_first(pd.Series(new_sup_ids))\n",
      "C:\\Users\\Kmaun\\AppData\\Local\\Temp\\ipykernel_35008\\1365823226.py:314: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  supplier_orders_data_df['supplier_purchase_order_id'] = supplier_orders_data_df['supplier_purchase_order_id'].combine_first(pd.Series(new_sup_ids))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded C:\\\\supermarket\\\\transformed_data\\transformed_branch_26-10-2024.csv on 26-10-2024\n",
      "Successfully uploaded C:\\\\supermarket\\\\transformed_data\\transformed_customer_26-10-2024.csv on 26-10-2024\n",
      "Successfully uploaded C:\\\\supermarket\\\\transformed_data\\transformed_employee_26-10-2024.csv on 26-10-2024\n",
      "Successfully uploaded C:\\\\supermarket\\\\transformed_data\\transformed_inventory_26-10-2024.csv on 26-10-2024\n",
      "Successfully uploaded C:\\\\supermarket\\\\transformed_data\\transformed_supplier_26-10-2024.csv on 26-10-2024\n",
      "Successfully uploaded C:\\\\supermarket\\\\transformed_data\\transformed_order_26-10-2024.csv on 26-10-2024\n",
      "Successfully uploaded C:\\\\supermarket\\\\transformed_data\\transformed_product_in_customer_26-10-2024.csv on 26-10-2024\n",
      "Successfully uploaded C:\\\\supermarket\\\\transformed_data\\transformed_payment_26-10-2024.csv on 26-10-2024\n",
      "Successfully uploaded C:\\\\supermarket\\\\transformed_data\\transformed_product_in_supplier_order_26-10-2024.csv on 26-10-2024\n",
      "Successfully uploaded C:\\\\supermarket\\\\transformed_data\\transformed_product_26-10-2024.csv on 26-10-2024\n",
      "Successfully uploaded C:\\\\supermarket\\\\transformed_data\\transformed_product_order_26-10-2024.csv on 26-10-2024\n",
      "All transformed data files uploaded to Google Drive 'supermarket\\transformed_data' successfully.\n",
      "END Saving Transformed data. Time taken:0 minutes, 29 seconds, and 125 milliseconds.\n",
      "Loading Transformed data to 'supermarket' database...\n",
      "Successfully uploaded data to 'branch' table in 'supermarket' database on 26-10-2024.\n",
      "Successfully uploaded data to 'supplier' table in 'supermarket' database on 26-10-2024.\n",
      "Successfully uploaded data to 'customer' table in 'supermarket' database on 26-10-2024.\n",
      "Successfully uploaded data to 'employee' table in 'supermarket' database on 26-10-2024.\n",
      "Successfully uploaded data to 'payment' table in 'supermarket' database on 26-10-2024.\n",
      "Successfully uploaded data to 'product' table in 'supermarket' database on 26-10-2024.\n",
      "Successfully uploaded data to 'order' table in 'supermarket' database on 26-10-2024.\n",
      "Successfully uploaded data to 'inventory' table in 'supermarket' database on 26-10-2024.\n",
      "Successfully uploaded data to 'product_in_customer_order' table in 'supermarket' database on 26-10-2024.\n",
      "Successfully uploaded data to 'product_order' table in 'supermarket' database on 26-10-2024.\n",
      "Successfully uploaded data to 'product_in_supplier_order' table in 'supermarket' database on 26-10-2024.\n",
      "END Loading Transformed data to 'supermarket' database. Time taken: 0 minutes, 0 seconds, and 606 milliseconds.\n",
      "Complete report e-mail sent successfully!\n",
      "END ELT process on 26-10-2024. Time taken: 0 minutes, 53 seconds, and 975 milliseconds.\n"
     ]
    }
   ],
   "source": [
    "### Start here \n",
    "if __name__ == '__main__':\n",
    "    ####### Email Config########\n",
    "    recipients = [\"kittisak.maungmanee@gmail.com\"]\n",
    "    ############################    \n",
    "    from datetime import datetime\n",
    "    import logging\n",
    "    import time\n",
    "    \n",
    "    etl_start_time = time.time()\n",
    "    \n",
    "    # Set up logging\n",
    "    logging.basicConfig(filename=r'C:\\supermarket\\log\\ELT_log.txt', level=logging.INFO,\n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    ######## Check Storage ##########\n",
    "    ## local storage\n",
    "    check_disk_space() \n",
    "\n",
    "    ## Google Drive\n",
    "    drive_service = authenticate_google_drive()\n",
    "    check_google_drive_space(drive_service)\n",
    "    #################################\n",
    "    # date format\n",
    "    record_date = datetime.now().strftime(\"%d-%m-%Y\")  \n",
    "\n",
    "    ####### START ELT process #######\n",
    "    print(f\"Starting ELT process on {record_date} ....\")\n",
    "    logging.info(f\"Starting ELT process on {record_date} ....\")\n",
    "    ############## EXTRACT DATA ###################\n",
    "    data_extraction_start_time = time.time()\n",
    "    # branches_df \n",
    "    # customers_df \n",
    "    # employees_df \n",
    "    # inventory_df \n",
    "    # suppliers_df\n",
    "    # payments_df\n",
    "    # order_df\n",
    "    # product_in_customers_df\n",
    "    # product_df\n",
    "    \n",
    "    data_generators = [\n",
    "    (generate_fake_branch_data, 20, 'branches_df', \"Generated branch data\"),\n",
    "    (generate_fake_customers_data, 1000, 'customers_df', \"Generated customer data\"),\n",
    "    (generate_employees_data, 100, 'employees_df', \"Generated employee data\"),\n",
    "    (create_inventory_data, (20, 66), 'inventory_df', \"Created inventory data\"),\n",
    "    (generate_fake_supplier_data, 20, 'suppliers_df', \"Generated supplier data\"),\n",
    "    (generate_all_products_data, None, 'product_df', \"Generated product data\"),\n",
    "    (generate_mock_orders, (random.randint(500, 1500)), 'orders_df', \"Generated mock order data\"),\n",
    "    (generate_mock_supplier_orders, (random.randint(50, 300)), 'supplier_order_df', \"Generated mock supplier order data\")\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        print(f\"Extracting data from python faker() generated on {record_date} ....\")\n",
    "        logging.info(f\"Extracting data from python faker() generated on {record_date} ....\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        branches = generate_fake_branch_data(20)  # number of records\n",
    "        branches_df = pd.DataFrame(branches)\n",
    "        logging.info(\"Generated branch data and created DataFrame.\")\n",
    "\n",
    "        customers = generate_fake_customers_data(1000)\n",
    "        customers_df = pd.DataFrame(customers)\n",
    "        logging.info(\"Generated customer data and created DataFrame.\")\n",
    "\n",
    "        employees = generate_employees_data(100)\n",
    "        employees_df = pd.DataFrame(employees)\n",
    "        logging.info(\"Generated employee data and created DataFrame.\")\n",
    "\n",
    "        inventory_df = create_inventory_data(20, 66)\n",
    "        logging.info(\"Created inventory data.\")\n",
    "\n",
    "        suppliers = generate_fake_supplier_data(20)\n",
    "        suppliers_df = pd.DataFrame(suppliers)\n",
    "        logging.info(\"Generated supplier data and created DataFrame.\")\n",
    "\n",
    "        products = generate_all_products_data()\n",
    "        product_df = pd.DataFrame(products)\n",
    "        logging.info(\"Generated product data and created DataFrame.\")\n",
    "\n",
    "        customer_order_data = generate_mock_orders(random.randint(500, 1500))\n",
    "        customer_order_df = pd.DataFrame(customer_order_data)\n",
    "        logging.info(\"Generated mock order data and created DataFrame.\")\n",
    "\n",
    "        supplier_order_data = generate_mock_supplier_orders(random.randint(50, 300))\n",
    "        supplier_order_df = pd.DataFrame(supplier_order_data)\n",
    "        logging.info(\"Generated mock supplier order data and created DataFrame.\")\n",
    "        \n",
    "        data_extraction_end_time = time.time()\n",
    "        data_extraction_total_duration = data_extraction_end_time - data_extraction_start_time \n",
    "        data_extraction_minutes = int(data_extraction_total_duration // 60)\n",
    "        data_extraction_seconds = int(data_extraction_total_duration % 60)\n",
    "        data_extraction_milliseconds = int((data_extraction_total_duration - int(data_extraction_total_duration)) * 1000)\n",
    "        \n",
    "        print(f\"Extracting data from python faker() successfully.  Time taken:{data_extraction_minutes} minutes, {data_extraction_seconds} seconds, and {data_extraction_milliseconds} milliseconds.\")\n",
    "        logging.info(f\"Extracting data from python faker() successfully. Time taken:{data_extraction_minutes} minutes, {data_extraction_seconds} seconds, and {data_extraction_milliseconds} milliseconds.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred | ERROR: {e}\")\n",
    "        print(f\"Failed to generate data| ERROR: {e}\")\n",
    "        send_failed_email(recipients,\"Failed to generate data\")\n",
    "        raise\n",
    "    \n",
    "    ########### End Extract Data ###################\n",
    "\n",
    "    ############################################## Load Raw Data ############################################   \n",
    "    \n",
    "    ############# Save raw data to Local Storage #############\n",
    "    \n",
    "\n",
    "    ### Save Raw_data to local storage ###\n",
    "    print(\"Saving Raw data to 'C:\\\\supermarket\\\\raw_data'...\")\n",
    "    logging.info(\"Saving Raw data to 'C:\\\\supermarket\\\\raw_data'...\")\n",
    "    # file location to save\n",
    "    raw_file_path = r'C:\\\\supermarket\\\\raw_data'\n",
    "    # End time\n",
    "    raw_data_start_time = time.time()\n",
    "    # List of tuples with DataFrame names and their respective filenames\n",
    "    raw_to_local_df = [\n",
    "        (branches_df, 'raw_branch'),\n",
    "        (customers_df, 'raw_customer'),\n",
    "        (employees_df, 'raw_employee'),\n",
    "        (inventory_df, 'raw_inventory'),\n",
    "        (suppliers_df, 'raw_supplier'),\n",
    "        (customer_order_df, 'raw_customer_order'),\n",
    "        (product_df, 'raw_product'),\n",
    "        (supplier_order_df, 'raw_supplier_order_data')\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Loop through each DataFrame and its corresponding filename\n",
    "        for df, name in raw_to_local_df:\n",
    "            file_path = f'{raw_file_path}\\\\{name}_{record_date}.csv'\n",
    "            df.to_csv(file_path, index=False)\n",
    "            print(f\"{name}_{record_date} saved successfully.\")\n",
    "            logging.info(f\"{name}_{record_date} saved successfully.\")\n",
    "        \n",
    "        print(f\"All raw data files have been saved to {raw_file_path} on {record_date} successfully.\")\n",
    "        logging.info(f\"All raw data files have been saved to {raw_file_path} on {record_date} successfully.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save raw data files in local storage on {record_date} ERROR: {e}\")\n",
    "        logging.error(f\"Failed to save raw data files in local storage on {record_date} ERROR: {e}\")\n",
    "        send_failed_email(recipients,f\"Failed to save raw data files in local storage on {record_date}.\")\n",
    "        raise\n",
    "    print(\"END Saving Raw data to 'C:\\\\supermarket\\\\raw_data'...\")\n",
    "    logging.info(\"END Saving Raw data to 'C:\\\\supermarket\\\\raw_data'...\")\n",
    "    ##############################################  End Save raw data to Local Storage ########################\n",
    "\n",
    "    ################################ Save Raw Data to Google Drive ##########################################\n",
    "    print(\"Saving Raw data files to Google Drive...\")\n",
    "    logging.info(\"Saving Raw data files to Google Drive...\")\n",
    "\n",
    "\n",
    "    \n",
    "    raw_branches_csv = f'{raw_file_path}\\\\raw_branch_{record_date}.csv'\n",
    "    raw_customers_csv = f'{raw_file_path}\\\\raw_customer_{record_date}.csv'\n",
    "    raw_employees_csv = f'{raw_file_path}\\\\raw_employee_{record_date}.csv'\n",
    "    raw_inventory_csv = f'{raw_file_path}\\\\raw_inventory_{record_date}.csv'\n",
    "    raw_supplier_csv = f'{raw_file_path}\\\\raw_supplier_{record_date}.csv'\n",
    "    raw_customer_order_csv = f'{raw_file_path}\\\\raw_customer_order_{record_date}.csv'\n",
    "    raw_product_csv = f'{raw_file_path}\\\\raw_product_{record_date}.csv'\n",
    "    raw_supplier_order_csv = f'{raw_file_path}\\\\raw_supplier_order_data_{record_date}.csv'\n",
    "    \n",
    "    try:\n",
    "        print(\"Getting Google Drive folder id... \")\n",
    "        logging.info(\"Getting Google Drive folder id...\")\n",
    "\n",
    "        # Retrieve folder IDs and log/print the process\n",
    "        main_folder_id = \"1pg99tJ7dIaPyEb8vS9VvWTxgHGzk-BD3\" # main_folder\n",
    "    \n",
    "        super_backup_id = get_folder_id_by_name(\"supermarket\", main_folder_id)\n",
    "        logging.info(f\"Successfully retrieved 'supermarket' folder ID: {super_backup_id}\")\n",
    "        print(f\"Successfully retrieved 'supermarket' folder ID: {super_backup_id}\")\n",
    "        \n",
    "        log_backup_id = get_folder_id_by_name(\"log\", super_backup_id)\n",
    "        logging.info(f\"Successfully retrieved 'log' folder ID: {log_backup_id}\")\n",
    "        print(f\"Successfully retrieved 'log' folder ID: {log_backup_id}\")\n",
    "        \n",
    "        raw_backup_id = get_folder_id_by_name(\"raw_data\", super_backup_id)\n",
    "        logging.info(f\"Successfully retrieved 'raw_data' folder ID: {raw_backup_id}\")\n",
    "        print(f\"Successfully retrieved 'raw_data' folder ID: {raw_backup_id}\")\n",
    "        \n",
    "        transform_backup_id = get_folder_id_by_name(\"transformed_data\", super_backup_id)\n",
    "        logging.info(f\"Successfully retrieved 'transformed_data' folder ID: {transform_backup_id}\")\n",
    "        print(f\"Successfully retrieved 'transformed_data' folder ID: {transform_backup_id}\")\n",
    "\n",
    "        print(\"Successfully get the folder id in Google Drive\")\n",
    "        logging.info(\"Successfully get the folder id in Google Drive\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get the folder id in Google | An error occurred:{e}\")\n",
    "        logging.error(f\"Failed to get the folder id in Google | An error occurred:{e}\")\n",
    "        send_failed_email(recipients,f\"Failed to get the folder id in Google\")\n",
    "        raise\n",
    "    # files to upload to google drive    \n",
    "    files = [\n",
    "        (raw_branches_csv, 'raw_branches_csv'),\n",
    "        (raw_customers_csv, 'raw_customers_csv'),\n",
    "        (raw_employees_csv, 'raw_employees_csv'),\n",
    "        (raw_inventory_csv, 'raw_inventory_csv'),\n",
    "        (raw_supplier_csv, 'raw_supplier_csv'),\n",
    "        (raw_customer_order_csv, 'raw_orders_csv'),\n",
    "        (raw_product_csv, 'raw_product_csv'),\n",
    "        (raw_supplier_order_csv, 'raw_supplier_order_csv')\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        print(\"Uploading raw data files to Google Drive 'supermarket\\\\raw_data'...\")\n",
    "        logging.info(\"Uploading raw data files to Google Drive 'supermarket\\\\raw_data'...\")\n",
    "\n",
    "        # Loop through each file and upload it to Google Drive\n",
    "        for file, file_name in files:\n",
    "            upload_file_to_drive(file, raw_backup_id)\n",
    "            message = f\"Successfully uploaded {file_name} to Google Drive on {record_date}\"\n",
    "            print(message)\n",
    "            logging.info(message)\n",
    "        \n",
    "        print(f\"All raw data files on {record_date} saved to Google Drive successfully.\")\n",
    "        logging.info(f\"All raw data files on {record_date} saved to Google Drive successfully.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save raw data files to Google Drive: {e}\")\n",
    "        logging.error(f\"Failed to save raw data files to Google Drive: {e}\")\n",
    "        send_failed_email(recipients,\"Failed to save raw data files to Google Drive\")\n",
    "        raise\n",
    "    # End time\n",
    "    raw_data_end_time = time.time()\n",
    "    raw_data_total_duration = raw_data_end_time - raw_data_start_time\n",
    "    raw_data_minutes = int(raw_data_total_duration // 60)\n",
    "    raw_data_seconds = int(raw_data_total_duration % 60)\n",
    "    raw_data_milliseconds = int((raw_data_total_duration - int(raw_data_total_duration)) * 1000)\n",
    "    print(f\"END Saving Raw data. Time taken:{raw_data_minutes} minutes, {raw_data_seconds} seconds, and {raw_data_milliseconds} milliseconds.\")\n",
    "    logging.info(f\"END Saving Raw data. Time taken:{raw_data_minutes} minutes, {raw_data_seconds} seconds, and {raw_data_milliseconds} milliseconds.\")\n",
    "    ################################################ END Save Raw Data to Google Drive ###################################\n",
    "    \n",
    "    ################################################ Transformed Data ########################################\n",
    "    ####### Order table #########\n",
    "    print(\"Transforming data....\")\n",
    "    logging.info(\"Transforming data....\")\n",
    "    \n",
    "    # STart time\n",
    "    transfomed_data_start_time = time.time()\n",
    "    try:   \n",
    "        # Copying the original DataFrame\n",
    "        main_orders_df = orders_df.copy()\n",
    "\n",
    "        # Remove customer_id randomly\n",
    "        num_nulls = int(len(main_orders_df) * round(random.uniform(0, 0.5), 4))\n",
    "        indices_for_nulls = random.sample(range(len(main_orders_df)), num_nulls)\n",
    "        main_orders_df.loc[indices_for_nulls, 'customer_id'] = None\n",
    "\n",
    "        # Insert a new column for payment_id with NaN values\n",
    "        main_orders_df.insert(loc=1, column='payment_id', value=np.nan)\n",
    "        \n",
    "\n",
    "        # Get the latest payment_id from the payments table\n",
    "        max_order_id = get_latest_id('payment_id', 'payments')\n",
    "        if max_order_id is None:\n",
    "            max_order_id = 0\n",
    "\n",
    "        # Fill missing payment_ids\n",
    "        num_missing = main_orders_df['payment_id'].isna().sum()\n",
    "        new_payment_ids = list(range(max_order_id + 1, max_order_id + 1 + num_missing))\n",
    "\n",
    "        # Exclude empty items from the new_payment_ids before combining\n",
    "        if new_payment_ids:\n",
    "            main_orders_df['payment_id'] = main_orders_df['payment_id'].combine_first(pd.Series(new_payment_ids))\n",
    "\n",
    "        # Handle order_id for orders table\n",
    "        max_payment_id = get_latest_id('order_id', 'customer_order')\n",
    "        if max_payment_id is None:\n",
    "            max_payment_id = 0\n",
    "\n",
    "        # Fill missing order_ids\n",
    "        num_missing_order = main_orders_df['order_id'].isna().sum()\n",
    "        new_order_ids = list(range(max_payment_id + 1, max_payment_id + 1 + num_missing_order))\n",
    "\n",
    "        # Exclude empty items from new_order_ids before combining\n",
    "        if new_order_ids:\n",
    "            main_orders_df['order_id'] = main_orders_df['order_id'].combine_first(pd.Series(new_order_ids))\n",
    "\n",
    "        # For supplier_orders_data_df, handle supplier_purchase_order_id similarly\n",
    "        num_missing_supplier = supplier_orders_data_df['supplier_purchase_order_id'].isna().sum()\n",
    "        new_sup_ids = list(range(max_order_id + 1, max_order_id + 1 + num_missing_supplier))\n",
    "\n",
    "        # Exclude empty items from new_sup_ids before combining\n",
    "        if new_sup_ids:\n",
    "            supplier_orders_data_df['supplier_purchase_order_id'] = supplier_orders_data_df['supplier_purchase_order_id'].combine_first(pd.Series(new_sup_ids))\n",
    "\n",
    "        # Split data into 3 tables as before\n",
    "        ## orders table\n",
    "        order_df = main_orders_df[['order_id', 'customer_id', 'payment_id', 'employee_id', 'branch_id']]\n",
    "\n",
    "        ## product_in_customers_order table\n",
    "        product_in_customers_df = main_orders_df[['order_id', 'product_id', 'order_date', 'amount', 'quantities']]\n",
    "\n",
    "        ## payments table\n",
    "        payments_df = main_orders_df[['payment_id', 'payment_method', 'transaction_id', 'billing_address', 'billing_city', 'billing_country']]\n",
    "\n",
    "        ############ END order_table #############\n",
    "\n",
    "        ######### Supplier order ##########\n",
    "        max_supplier_id = get_latest_id('supplier_purchase_order_id' , 'product_order')\n",
    "        # Set default max_payment_id if None\n",
    "        if max_supplier_id is None:\n",
    "            max_supplier_id = 0\n",
    "\n",
    "        # Check for missing payment_ids in the orders DataFrame\n",
    "        num_missing = supplier_orders_data_df['supplier_purchase_order_id'].isna().sum()\n",
    "\n",
    "        # Generate new payment IDs\n",
    "        new_sup_ids = range(max_supplier_id + 1, max_supplier_id + 1 + num_missing)\n",
    "\n",
    "        # Use combine_first to fill missing values in payment_id column\n",
    "        supplier_orders_data_df['supplier_purchase_order_id'] = supplier_orders_data_df['supplier_purchase_order_id'].combine_first(pd.Series(new_sup_ids))\n",
    "\n",
    "        products_in_supplier_orders = supplier_orders_data_df[['supplier_purchase_order_id','supplier_id','product_id','order_date','arrival_date','quantity','unit_price']]\n",
    "        products_orders = supplier_orders_data_df[['supplier_purchase_order_id', 'employee_id', 'branch_id']]\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to transform raw data: {e}\")\n",
    "        logging.error(f\"Failed to transform raw data: {e}\")\n",
    "        send_failed_email(recipients,\"Failed to transform raw data.\")\n",
    "        raise\n",
    "    # End time\n",
    "    transfomed_data_end_time = time.time()\n",
    "    transfomed_data_total_duration = transfomed_data_end_time - transfomed_data_start_time\n",
    "    transfomed_data_minutes = int(transfomed_data_total_duration // 60)\n",
    "    transfomed_data_seconds = int(transfomed_data_total_duration % 60)\n",
    "    transfomed_data_milliseconds = int((transfomed_data_total_duration - int(transfomed_data_total_duration)) * 1000)\n",
    "    \n",
    "    print(f\"END transforming data. Time taken:{transfomed_data_minutes} minutes, {transfomed_data_seconds} seconds, and {transfomed_data_milliseconds} milliseconds.\")\n",
    "    logging.info(f\"END transforming data. Time taken:{transfomed_data_minutes} minutes, {transfomed_data_seconds} seconds, and {transfomed_data_milliseconds} milliseconds.\")\n",
    "    ######### END Supplier order ##########\n",
    "    ################################################ END transformed Data ########################################\n",
    "\n",
    "    ############################################## LOAD transformed data #####################################################\n",
    "    print(\"Saving Transformed data...\")\n",
    "    logging.info(\"Saving Transformed data...\")\n",
    "    \n",
    "    transform_file_path = r'C:\\\\supermarket\\\\transformed_data'\n",
    "    # start time\n",
    "    transformed_save_2places_upload_start_time = time.time()\n",
    "    try:\n",
    "        print(f\"Saving Transformed data to {transform_file_path}\")\n",
    "        logging.info(f\"Saving Transformed data to {transform_file_path}\")\n",
    "\n",
    "        # Save each DataFrame as a CSV file in the specified directory\n",
    "        branches_df.to_csv(f'{transform_file_path}\\\\transformed_branch_{record_date}.csv', index=False)\n",
    "        customers_df.to_csv(f'{transform_file_path}\\\\transformed_customer_{record_date}.csv', index=False)\n",
    "        employees_df.to_csv(f'{transform_file_path}\\\\transformed_employee_{record_date}.csv', index=False)\n",
    "        inventory_df.to_csv(f'{transform_file_path}\\\\transformed_inventory_{record_date}.csv', index=False)\n",
    "        suppliers_df.to_csv(f'{transform_file_path}\\\\transformed_supplier_{record_date}.csv', index=False)\n",
    "        order_df.to_csv(f'{transform_file_path}\\\\transformed_order_{record_date}.csv', index=False)\n",
    "        product_in_customers_df.to_csv(f'{transform_file_path}\\\\transformed_product_in_customer_{record_date}.csv', index=False)\n",
    "        payments_df.to_csv(f'{transform_file_path}\\\\transformed_payment_{record_date}.csv', index=False)\n",
    "        products_in_supplier_orders.to_csv(f'{transform_file_path}\\\\transformed_product_in_supplier_order_{record_date}.csv', index=False)\n",
    "        product_df.to_csv(f'{transform_file_path}\\\\transformed_product_{record_date}.csv', index=False)\n",
    "        products_orders.to_csv(f'{transform_file_path}\\\\transformed_product_order_{record_date}.csv', index=False)\n",
    "        print(f\"Transformed data has been saved to {transform_file_path} successfully.\")\n",
    "        logging.info(f\"Transformed data has been saved to {transform_file_path} successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save data in {transform_file_path}: {e}\")\n",
    "        logging.error(f\"Failed to save data in {transform_file_path}: {e}\")\n",
    "        send_failed_email(recipients,f\"Failed to save data in {transform_file_path}\")\n",
    "        raise\n",
    "    print(f\"END Saving Transformed data to {transform_file_path}'\")\n",
    "    logging.info(f\"END Saving Transformed data to {transform_file_path}\")\n",
    "    ########### Google Drive ############\n",
    "    \n",
    "    ### Transformed Data Upload ### \n",
    "    transformed_branches_csv = f'{transform_file_path}\\\\transformed_branch_{record_date}.csv'\n",
    "    transformed_customers_csv = f'{transform_file_path}\\\\transformed_customer_{record_date}.csv'\n",
    "    transformed_employees_csv = f'{transform_file_path}\\\\transformed_employee_{record_date}.csv'\n",
    "    transformed_inventory_csv = f'{transform_file_path}\\\\transformed_inventory_{record_date}.csv'\n",
    "    transformed_supplier_csv = f'{transform_file_path}\\\\transformed_supplier_{record_date}.csv'\n",
    "    transformed_order_csv = f'{transform_file_path}\\\\transformed_order_{record_date}.csv'\n",
    "    transformed_product_in_customer_csv = f'{transform_file_path}\\\\transformed_product_in_customer_{record_date}.csv'\n",
    "    transformed_payment_csv = f'{transform_file_path}\\\\transformed_payment_{record_date}.csv'\n",
    "    transformed_products_in_supplier_order_csv = f'{transform_file_path}\\\\transformed_product_in_supplier_order_{record_date}.csv'\n",
    "    transformed_product_csv = f'{transform_file_path}\\\\transformed_product_{record_date}.csv'\n",
    "    transformed_products_orders_csv = f'{transform_file_path}\\\\transformed_product_order_{record_date}.csv'\n",
    "    \n",
    "\n",
    "\n",
    "    try:\n",
    "        print(\"Uploading transformed data files to Google Drive 'supermarket\\\\transformed_data'...\")\n",
    "        logging.info(\"Uploading transformed data files to Google Drive 'supermarket\\\\transformed_data'...\")\n",
    "        \n",
    "        transformed_files = [\n",
    "            transformed_branches_csv,\n",
    "            transformed_customers_csv,\n",
    "            transformed_employees_csv,\n",
    "            transformed_inventory_csv,\n",
    "            transformed_supplier_csv,\n",
    "            transformed_order_csv,\n",
    "            transformed_product_in_customer_csv,\n",
    "            transformed_payment_csv,\n",
    "            transformed_products_in_supplier_order_csv,\n",
    "            transformed_product_csv,\n",
    "            transformed_products_orders_csv\n",
    "        ]\n",
    "\n",
    "        # Upload each file and log the result\n",
    "        for file in transformed_files:\n",
    "            upload_file_to_drive(file, transform_backup_id)\n",
    "            print(f\"Successfully uploaded {file} on {record_date}\")\n",
    "            logging.info(f\"Successfully uploaded {file} on {record_date}\")\n",
    "                \n",
    "        print(\"All transformed data files uploaded to Google Drive 'supermarket\\\\transformed_data' successfully.\")\n",
    "        logging.info(\"All transformed data files uploaded to Google Drive 'supermarket\\\\transformed_data' successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload transformed data files: {e}\")\n",
    "        logging.error(f\"Failed to upload transformed data files: {e}\")\n",
    "        send_failed_email(recipients,\"Failed to upload transformed data files\")\n",
    "        raise\n",
    "    # end time\n",
    "    transformed_save_2places_upload_end_time = time.time()\n",
    "    transformed_save_2places_upload_total_duration = transformed_save_2places_upload_end_time - transformed_save_2places_upload_start_time\n",
    "    transformed_save_2places_upload_minutes = int(transformed_save_2places_upload_total_duration // 60)\n",
    "    transformed_save_2places_upload_seconds = int(transformed_save_2places_upload_total_duration % 60)\n",
    "    transformed_save_2places_upload_milliseconds = int((transformed_save_2places_upload_total_duration - int(transformed_save_2places_upload_total_duration)) * 1000)\n",
    "    print(f\"END Saving Transformed data. Time taken:{transformed_save_2places_upload_minutes} minutes, {transformed_save_2places_upload_seconds} seconds, and {transformed_save_2places_upload_milliseconds} milliseconds.\")\n",
    "    logging.info(f\"END Saving Transformed data. Time Taken:{transformed_save_2places_upload_minutes} minutes, {transformed_save_2places_upload_seconds} seconds, and {transformed_save_2places_upload_milliseconds} milliseconds.\")\n",
    "    \n",
    "    ############################################## END LOAD transformed data #####################################################\n",
    "    \n",
    "    ############################################ Load dataframes to database ######################################################\n",
    "    print(\"Loading Transformed data to 'supermarket' database...\")\n",
    "    logging.info(\"Loading Transformed data to 'supermarket' database...\")\n",
    "    \n",
    "    # start time\n",
    "    append_to_db_start_time = time.time()\n",
    "\n",
    "    try:   \n",
    "        dataframes_to_append = [\n",
    "            (branches_df, 'branch'),\n",
    "            (suppliers_df, 'supplier'),\n",
    "            (customers_df, 'customer'),\n",
    "            (employees_df, 'employee'),\n",
    "            (payments_df, 'payment'),\n",
    "            (products_df, 'product'),\n",
    "            (order_df, 'order'),\n",
    "            (inventory_df, 'inventory'),\n",
    "            (product_in_customers_df, 'product_in_customer_order'),\n",
    "            (products_orders, 'product_order'),\n",
    "            (products_in_supplier_orders, 'product_in_supplier_order')\n",
    "        ]\n",
    "        config = get_db_config(\"mysql_db_acc_detail.json\")\n",
    "        host=config['host'] \n",
    "        database=config['database'] \n",
    "        user=config['user'] \n",
    "        password=config['password']\n",
    "    \n",
    "        # Loop through the list and append each DataFrame to the corresponding table\n",
    "        for df, table_name in dataframes_to_append:\n",
    "            append_dataframe_to_mysql(df, table_name, host, database, user, password)\n",
    "            # Print and log messages without showing the DataFrame\n",
    "            print(f\"Successfully uploaded data to '{table_name}' table in 'supermarket' database on {record_date}.\")\n",
    "            logging.info(f\"Successfully uploaded data to '{table_name}' table in 'supermarket' database on {record_date}.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload transformed data to 'supermarket' database on {record_date}: {e}\")\n",
    "        logging.error(f\"Failed to upload transformed data to 'supermarket' database on {record_date}: {e}\")\n",
    "        send_failed_email(recipients,f\"Failed to upload transformed data to 'supermarket' database on {record_date}\")\n",
    "        raise\n",
    "        \n",
    "    # end time\n",
    "    append_to_db_end_time = time.time()\n",
    "    append_to_db_total_duration = append_to_db_end_time - append_to_db_start_time\n",
    "    append_to_db_minutes = int(append_to_db_total_duration // 60)\n",
    "    append_to_db_seconds = int(append_to_db_total_duration % 60)\n",
    "    append_to_db_milliseconds = int((append_to_db_total_duration - int(append_to_db_total_duration)) * 1000)\n",
    "\n",
    "\n",
    "    print(f\"END Loading Transformed data to 'supermarket' database. Time taken: {append_to_db_minutes} minutes, {append_to_db_seconds} seconds, and {append_to_db_milliseconds} milliseconds.\")\n",
    "    logging.info(f\"END Loading Transformed data to 'supermarket' database. Time taken: {append_to_db_minutes} minutes, {append_to_db_seconds} seconds, and {append_to_db_milliseconds} milliseconds.\") \n",
    "    ############################################ END LOAD dataframes to database ######################################################\n",
    "\n",
    "    etl_end_time = time.time()\n",
    "    \n",
    "    etl_total_duration = etl_end_time - etl_start_time\n",
    "\n",
    "    # Format the duration into minutes, seconds, and milliseconds\n",
    "    etl_minutes = int(etl_total_duration // 60)\n",
    "    etl_seconds = int(etl_total_duration % 60)\n",
    "    etl_milliseconds = int((etl_total_duration - int(etl_total_duration)) * 1000)\n",
    "    \n",
    "    ### Send email for ELT completion ###            \n",
    "    send_success_email(recipients,etl_minutes,etl_seconds)\n",
    "    print(f\"END ELT process on {record_date}. Time taken: {etl_minutes} minutes, {etl_seconds} seconds, and {etl_milliseconds} milliseconds.\")\n",
    "    logging.info(f\"END ELT process on {record_date}. Time taken: {etl_minutes} minutes, {etl_seconds} seconds, and {etl_milliseconds} milliseconds.\")\n",
    "    \n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to remove the old ETL log file in Google Drive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing the old ELT log file\n",
      "Failed to remove the old ETL log file in Google Drive.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'recipients' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      4\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemoving the old ELT log file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m delete_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mELT_log.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[43mlog_backup_id\u001b[49m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully removed the old ETL log file in Google Drive. Folder id :\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_backup_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'log_backup_id' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to remove the old ETL log file in Google Drive.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to remove the old ETL log file in Google Drive.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m     send_failed_email(\u001b[43mrecipients\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to remove the old ETL log file in Google Drive.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m## Backup ELT log file to Google Drive \u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'recipients' is not defined"
     ]
    }
   ],
   "source": [
    "# backup eltl log every 7 days\n",
    "try:\n",
    "    # Get today's date\n",
    "    today = datetime.datetime.today().day\n",
    "    \n",
    "    # Run backup if today is 7, 14, 21, or 28\n",
    "    if today in {7, 14, 21, 28}:\n",
    "        print(\"Updating Log file to Google Drive...\")\n",
    "        logging.info(\"Updating Log file to Google Drive...\")\n",
    "        \n",
    "        etl_log_file = r'C:\\supermarket\\log\\ELT_log.txt'\n",
    "        upload_file_to_drive(etl_log_file, log_backup_id)\n",
    "        \n",
    "        print(\"Successfully updated backup ELT log file to Google Drive.\")\n",
    "        logging.info(\"Successfully updated backup ELT log file to Google Drive.\")\n",
    "    else:\n",
    "        print(\"Backup not scheduled for today.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Failed to update backup ELT log file to Google Drive.\")\n",
    "    logging.error(\"Failed to update backup ELT log file to Google Drive.\")\n",
    "    send_failed_email(recipients, \"Failed to update backup ELT log file to Google Drive\")\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
